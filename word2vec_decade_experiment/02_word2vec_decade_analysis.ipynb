{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Word2Vec by Decades Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to calculate statistics on fully trained word2vec models trained in [01_word2vec_decade_runner.ipynb](01_word2vec_decade_runner.ipynb). The statistics calculated are the cosine distance between tokens on a global level and a local level. Cosine distance is a helpful metric as it isn't affected by the magnitude of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.549519Z",
     "start_time": "2021-05-12T03:46:15.782970Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import tqdm\n",
    "import plotnine as p9\n",
    "\n",
    "from biovectors_modules.word2vec_analysis_helper import get_global_local_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Parse Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.578408Z",
     "start_time": "2021-05-12T03:46:46.550986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Align word2vec Models since cutoff year\n",
    "year_cutoff = 2000\n",
    "latest_year = 2020\n",
    "aligned_model_file_path = f\"output/aligned_word_vectors_{year_cutoff}_{latest_year}.pkl\"\n",
    "aligned_models = dict()\n",
    "token_occurence_file = \"output/earliest_token_occurence.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.718406Z",
     "start_time": "2021-05-12T03:46:46.579822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('output/models/word2vec_2020.model'), PosixPath('output/models/word2vec_2019.model'), PosixPath('output/models/word2vec_2018.model'), PosixPath('output/models/word2vec_2017.model'), PosixPath('output/models/word2vec_2016.model'), PosixPath('output/models/word2vec_2015.model'), PosixPath('output/models/word2vec_2014.model'), PosixPath('output/models/word2vec_2013.model'), PosixPath('output/models/word2vec_2012.model'), PosixPath('output/models/word2vec_2011.model'), PosixPath('output/models/word2vec_2010.model'), PosixPath('output/models/word2vec_2009.model'), PosixPath('output/models/word2vec_2008.model'), PosixPath('output/models/word2vec_2007.model'), PosixPath('output/models/word2vec_2006.model'), PosixPath('output/models/word2vec_2005.model'), PosixPath('output/models/word2vec_2004.model'), PosixPath('output/models/word2vec_2003.model'), PosixPath('output/models/word2vec_2002.model'), PosixPath('output/models/word2vec_2001.model'), PosixPath('output/models/word2vec_2000.model')]\n"
     ]
    }
   ],
   "source": [
    "# Skip 2021 as that model is too small to analyze\n",
    "# Try again December 2021\n",
    "word_models = filter(\n",
    "    lambda x: int(x.stem.split(\"_\")[1]) >= year_cutoff\n",
    "    and int(x.stem.split(\"_\")[1]) != 2021,\n",
    "    list(Path(\"output/models\").rglob(\"*model\")),\n",
    ")\n",
    "word_models = sorted(word_models, key=lambda x: int(x.stem.split(\"_\")[1]), reverse=True)\n",
    "print(word_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.792540Z",
     "start_time": "2021-05-12T03:46:46.720062Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(token_occurence_file).exists():\n",
    "    earliest_token_occurence = dict()\n",
    "    for model in reversed(word_models):\n",
    "        year = model.stem.split(\"_\")[1]\n",
    "        model = Word2Vec.load(str(model))\n",
    "        for token in model.wv.vocab.keys():\n",
    "            if token not in earliest_token_occurence:\n",
    "                earliest_token_occurence[token] = f\"{year}\"\n",
    "            else:\n",
    "                earliest_token_occurence[token] += f\"|{year}\"\n",
    "        (\n",
    "            pd.DataFrame(\n",
    "                list(earliest_token_occurence.items()),\n",
    "                columns=[\"token\", \"year_occured\"],\n",
    "            ).to_csv(token_occurence_file, sep=\"\\t\", index=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Global and Local Distances Between Time Periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align Models via Orthogonal Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.835012Z",
     "start_time": "2021-05-12T03:46:46.793776Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "\n",
    "    for model in tqdm.tqdm(word_models):\n",
    "        year = model.stem.split(\"_\")[1]\n",
    "        word_model = Word2Vec.load(str(model))\n",
    "\n",
    "        if year == str(latest_year):\n",
    "            origin_tokens = sorted(list(set(word_model.wv.vocab.keys())))\n",
    "            remaining_tokens = origin_tokens\n",
    "\n",
    "        else:\n",
    "            tokens = sorted(list(set(word_model.wv.vocab.keys())))\n",
    "            remaining_tokens = set(origin_tokens) & set(tokens)\n",
    "\n",
    "        data_records = []\n",
    "\n",
    "        for tok in remaining_tokens:\n",
    "            data_entry = dict(\n",
    "                zip(\n",
    "                    [f\"feat_{idx}\" for idx in range(len(word_model.wv[tok]))],\n",
    "                    word_model.wv[tok],\n",
    "                )\n",
    "            )\n",
    "            data_entry[\"token\"] = tok\n",
    "            data_records.append(data_entry)\n",
    "\n",
    "        aligned_models[year] = pd.DataFrame.from_records(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.909896Z",
     "start_time": "2021-05-12T03:46:46.836123Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "    years_analyzed = sorted(list(aligned_models.keys()), reverse=True)[1:]\n",
    "    latest_year = str(latest_year)\n",
    "\n",
    "    origin_df = aligned_models[latest_year].set_index(\"token\")\n",
    "\n",
    "    # Years must be in sorted descending order\n",
    "    for year in tqdm.tqdm(years_analyzed):\n",
    "        tokens = sorted(aligned_models[year].token.tolist())\n",
    "        needs_aligned_df = aligned_models[year].set_index(\"token\")\n",
    "\n",
    "        # align A to B subject to transition matrix being\n",
    "        # orthogonal to preserve the cosine similarities\n",
    "        translation_matrix, scale = orthogonal_procrustes(\n",
    "            needs_aligned_df.loc[tokens].values,\n",
    "            origin_df.loc[tokens].values,\n",
    "        )\n",
    "\n",
    "        # Matrix Multiplication to project year onto 2020\n",
    "        aligned_word_matrix = needs_aligned_df.loc[tokens].values @ translation_matrix\n",
    "\n",
    "        corrected_df = pd.DataFrame(aligned_word_matrix)\n",
    "        corrected_df.columns = needs_aligned_df.columns.tolist()\n",
    "        aligned_models[year] = corrected_df.assign(token=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:46:46.968861Z",
     "start_time": "2021-05-12T03:46:46.912474Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "    pickle.dump(aligned_models, open(aligned_model_file_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Global and Local Distances between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T03:47:30.560749Z",
     "start_time": "2021-05-12T03:46:46.970570Z"
    }
   },
   "outputs": [],
   "source": [
    "aligned_models = pickle.load(open(aligned_model_file_path, \"rb\"))\n",
    "years_analyzed = sorted(list(aligned_models.keys()), reverse=True)\n",
    "origin_year = years_analyzed[-6]  # grab the earliest year to date\n",
    "n_neighbors = 25\n",
    "year_distance_folder = f\"year_distances_{year_cutoff+5}_{latest_year}_replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-12T03:46:15.825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [16:34:00<120:24:31, 33343.97s/it]"
     ]
    }
   ],
   "source": [
    "for key in tqdm.tqdm(years_analyzed[:-6]):\n",
    "    tokens_to_compare = sorted(aligned_models[origin_year].token.tolist())\n",
    "    shared_tokens = set(tokens_to_compare) & set(aligned_models[key].token.tolist())\n",
    "    shared_tokens = sorted(list(shared_tokens))\n",
    "\n",
    "    total_distance = get_global_local_distance(\n",
    "        aligned_models[origin_year],\n",
    "        aligned_models[key],\n",
    "        shared_tokens,\n",
    "        neighbors=n_neighbors,\n",
    "        n_jobs=3,\n",
    "    )\n",
    "\n",
    "    Path(f\"output/{year_distance_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "    label = f\"{origin_year}_{key}\"\n",
    "    output_filepath = Path(f\"output/{year_distance_folder}\") / Path(f\"{label}_dist.tsv\")\n",
    "\n",
    "    (\n",
    "        total_distance.assign(\n",
    "            shift=lambda x: x.global_dist.values - x.local_dist.values\n",
    "        ).to_csv(str(output_filepath), index=False, sep=\"\\t\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:biovectors]",
   "language": "python",
   "name": "conda-env-biovectors-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
