{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Word2Vec by Decades Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T16:52:36.284641Z",
     "start_time": "2021-04-24T16:52:34.463898Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import tqdm\n",
    "import plotnine as p9\n",
    "\n",
    "from biovectors_modules.word2vec_analysis_helper import (\n",
    "    get_global_distance,\n",
    "    get_local_distance,\n",
    "    window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Parse Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T16:52:36.311778Z",
     "start_time": "2021-04-24T16:52:36.285971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Align word2vec Models since cutoff year\n",
    "year_cutoff = 2000\n",
    "aligned_model_file_path = \"output/aligned_word_vectors.pkl\"\n",
    "token_occurence_file = \"output/earliest_token_occurence.tsv\"\n",
    "year_distance_folder = \"year_distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T16:52:36.373550Z",
     "start_time": "2021-04-24T16:52:36.313466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Skip 2021 as that model is too small to analyze\n",
    "# Try again December 2021\n",
    "word_models = filter(\n",
    "    lambda x: int(x.stem.split(\"_\")[1]) >= year_cutoff\n",
    "    and int(x.stem.split(\"_\")[1]) != 2021,\n",
    "    list(Path(\"output/models\").rglob(\"*model\")),\n",
    ")\n",
    "word_models = sorted(word_models, key=lambda x: int(x.stem.split(\"_\")[1]), reverse=True)\n",
    "print(word_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T16:52:36.464901Z",
     "start_time": "2021-04-24T16:52:36.374848Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(token_occurence_file).exists():\n",
    "    word_model_dict = dict()\n",
    "    earliest_token_occurence = dict()\n",
    "    for model in reversed(word_models):\n",
    "        year = int(model.stem.split(\"_\")[1])\n",
    "        model = Word2Vec.load(str(model))\n",
    "        for token in model.wv.vocab.keys():\n",
    "            if token not in earliest_token_occurence:\n",
    "                earliest_token_occurence[token] = f\"{year}\"\n",
    "            else:\n",
    "                earliest_token_occurence[token] += f\"|{year}\"\n",
    "        (\n",
    "            pd.DataFrame(\n",
    "                list(earliest_token_occurence.items()),\n",
    "                columns=[\"token\", \"year_occured\"],\n",
    "            ).to_csv(token_occurence_file, sep=\"\\t\", index=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:19:03.985372Z",
     "start_time": "2021-04-15T03:19:03.964341Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "    word_model_dict = dict()\n",
    "    shared_tokens = set()\n",
    "    for model in word_models:\n",
    "        year = int(model.stem.split(\"_\")[1])\n",
    "        word_model_dict[year] = Word2Vec.load(str(model))\n",
    "        if len(shared_tokens) == 0:\n",
    "            shared_tokens = set(word_model_dict[year].wv.vocab.keys())\n",
    "        else:\n",
    "            shared_tokens &= set(word_model_dict[year].wv.vocab.keys())\n",
    "\n",
    "    shared_tokens = sorted(list(shared_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Global and Local Distances Between Time Periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align Models via Orthogonal Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:19:04.003606Z",
     "start_time": "2021-04-15T03:19:03.986582Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "    years_analyzed = sorted(list(word_model_dict.keys()), reverse=True)\n",
    "    latest_year = years_analyzed[0]\n",
    "    aligned_models = {}\n",
    "\n",
    "    # Years must be in sorted descending order\n",
    "    for year_pair in window(years_analyzed):\n",
    "\n",
    "        year_pair_label = \"_\".join(map(str, year_pair[::-1]))\n",
    "\n",
    "        if year_pair[0] == latest_year:\n",
    "            aligned_models[str(year_pair[0])] = word_model_dict[year_pair[0]].wv[\n",
    "                shared_tokens\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "\n",
    "            aligned_models[str(year_pair[0])] = word_model_dict[year_pair[0]].wv[\n",
    "                shared_tokens\n",
    "            ]\n",
    "\n",
    "            translation_matrix, scale = orthogonal_procrustes(\n",
    "                word_model_dict[year_pair[0]].wv[shared_tokens],\n",
    "                word_model_dict[latest_year].wv[shared_tokens],\n",
    "            )\n",
    "\n",
    "        translation_matrix, scale = orthogonal_procrustes(\n",
    "            word_model_dict[year_pair[1]].wv[shared_tokens],\n",
    "            word_model_dict[latest_year].wv[shared_tokens],\n",
    "        )\n",
    "\n",
    "        aligned_models[str(year_pair[1])] = (\n",
    "            word_model_dict[year_pair[1]].wv[shared_tokens] @ translation_matrix\n",
    "        )\n",
    "\n",
    "    aligned_models[\"shared_tokens\"] = shared_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:19:04.019631Z",
     "start_time": "2021-04-15T03:19:04.004806Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Path(aligned_model_file_path).exists():\n",
    "    pickle.dump(aligned_models, open(aligned_model_file_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Global and Local Distances between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:19:04.571913Z",
     "start_time": "2021-04-15T03:19:04.020727Z"
    }
   },
   "outputs": [],
   "source": [
    "aligned_models = pickle.load(open(aligned_model_file_path, \"rb\"))\n",
    "years_analyzed = sorted(list(aligned_models.keys()), reverse=True)[1:]\n",
    "n_neighbors = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:57:18.893218Z",
     "start_time": "2021-04-15T03:19:04.573516Z"
    }
   },
   "outputs": [],
   "source": [
    "shared_tokens = sorted(aligned_models[\"shared_tokens\"])\n",
    "for key in tqdm.tqdm(window(years_analyzed)):\n",
    "\n",
    "    global_distance = get_global_distance(\n",
    "        aligned_models[key[0]], aligned_models[key[1]], aligned_models[\"shared_tokens\"]\n",
    "    )\n",
    "\n",
    "    local_distance = get_local_distance(\n",
    "        aligned_models[key[0]],\n",
    "        aligned_models[key[1]],\n",
    "        aligned_models[\"shared_tokens\"],\n",
    "        neighbors=n_neighbors,\n",
    "    )\n",
    "\n",
    "    label = \"_\".join(reversed(key))\n",
    "\n",
    "    (\n",
    "        global_distance.merge(local_distance)\n",
    "        .assign(shift=lambda x: x.global_dist.values - x.local_dist.values)\n",
    "        .to_csv(\n",
    "            f\"output/{year_distance_folder}/{label}_dist.tsv\", sep=\"\\t\", index=False\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:biovectors]",
   "language": "python",
   "name": "conda-env-biovectors-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
